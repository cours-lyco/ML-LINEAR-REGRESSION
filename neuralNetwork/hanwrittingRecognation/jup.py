{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "#from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "#import numpy as np\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import glob\n",
    "\n",
    "from tensorflow import keras\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, InputLayer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "#Python --version: Python 3.7 :: Anaconda, Inc.\n",
    "'-------------------------------------------------------'\n",
    "'               EXTERNAL PATHS                          '\n",
    "'-------------------------------------------------------'\n",
    "'''path of external device '''\n",
    "rootDir = \"/Volumes/Disk E/\"\n",
    "datasDir = rootDir + \"/crohme2016_inkml_datasets/CROHME2016_data/\"\n",
    "\n",
    "imagesDir = rootDir + \"/crohme2016_inkml_images/\"\n",
    "'''now get train valid test dir'''\n",
    "\n",
    "trainDir = datasDir + 'Task-2-Symbols/task2-trainSymb2014/'\n",
    "trainDirDatasets = trainDir + 'trainingSymbols/'\n",
    "trainDir_ground_truth = trainDir + 'trainingSymbols/iso_GT.txt'\n",
    "trainDatasImmagesDir = \"/Volumes/Disk E/crohme2016_inkml_images/train_images\"\n",
    "\n",
    "testDir = datasDir + 'Task-2-Symbols/task2-testSymbols2014/'\n",
    "testDir_ground_truth = testDir + \"testSymbols_2016_iso_GT.txt\"\n",
    "testDirDatasets = testDir + \"testSymbols/\"\n",
    "testDatasImmagesDir = \"/Volumes/Disk E/crohme2016_inkml_images/test_images\"\n",
    "\n",
    "valDir = datasDir + 'Task-2-Symbols/task2-validation-isolatedTest2013b/'\n",
    "valDirDatasets = datasDir + 'Task-2-Symbols/task2-validation-isolatedTest2013b/'\n",
    "valDir_ground_truth = valDirDatasets + 'iso_GT.txt'\n",
    "valDatasImmagesDir = \"/Volumes/Disk E/crohme2016_inkml_images/val_images\"\n",
    "\n",
    "models_save = \"/Volumes/Disk E/models_save/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "\n",
    "    differents_characters = 101\n",
    "    image_len = 28\n",
    "    X_train_len = 85802\n",
    "    X_test_len = 18435\n",
    "    X_val_len = 12504\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X_train, self.X_test, self.X_val = [], [], []\n",
    "        self.y_train, self.y_test, self.y_val = [], [], []\n",
    "\n",
    "        self.y_train_ordered_labels = []\n",
    "        self.y_test_ordered_labels = []\n",
    "        self.y_val_ordered_labels = []\n",
    "        \n",
    "        \n",
    "       \n",
    "        #self.gt = [ {sortedDictTrain}  , {sortedDictTest}, {sortedDictval} ]\n",
    "        #sortedDictTrain = ')' : [chemin de ttes les images de parenth√®ses], '9': [chemin de ttes les images de 9],  ...\n",
    "        self.gt = {} \n",
    "\n",
    "        '''first load grounth truth and prevent junk data to load '''\n",
    "        self.get_all_images_categorical()\n",
    "        '''second load images and prevent junk images  to load '''\n",
    "        self.get_all_images_Matrix()\n",
    "\n",
    "        self.X_train, self.X_test, self.X_val = np.array(self.X_train), np.array(self.X_test), np.array(self.X_val)\n",
    "        self.y_train, self.y_test, self.y_val = np.array(self.y_train), np.array(self.y_test), np.array(self.y_val)\n",
    "        \n",
    "        self.X_train, self.X_test, self.X_val= (self.X_train - np.mean(self.X_train))/np.std(self.X_train), (self.X_test - np.mean(self.X_test))/np.std(self.X_test) , (self.X_val - np.mean(self.X_val))/np.std(self.X_val)\n",
    "        \n",
    "        'normalize R G B: L=X_train R, A=X_train G, B=X_train B'\n",
    "        # L, A, B = imgLABs_arr[ :, :, :, 0 : 1 ], imgLABs_arr[ :, :, :, 1 : 2 ], imgLABs_arr[ :, :, :, 2 : 3 ]\n",
    "        #L_mean, L_std = np.mean( L ), np.std( L )\n",
    "        #A_mean, A_std = np.mean( A ), np.std( A )\n",
    "        #B_mean, B_std = np.mean( B ), np.std( B )\n",
    "        #L, A, B = ( L - L_mean ) / L_std, ( A - A_mean ) / A_std, ( B - B_mean ) / B_std\n",
    "        #AB = np.concatenate( ( A, B ), axis = 3)\n",
    "\n",
    "    def get_matrix_from_image(self, infilename):\n",
    "        img = Image.open(infilename)\n",
    "        img.load()\n",
    "        imageMatrix = np.asarray(img, dtype=\"float32\")/255.0\n",
    "        return imageMatrix\n",
    "    #------------------------------------------------------------------------------------------------------#\n",
    "    #                         GET TRAIN RESAUL\n",
    "    #-------------------------------------------------------------------------------------------------------#\n",
    "    def get_which_character(y):\n",
    "        pass\n",
    "    #------------------------------------------------------------------------------------------------------#\n",
    "    # result: X_train = [imageMatrix_iso0.png, imageMatrix_iso0.png, ........., imageMatrix_iso85802.png ]\n",
    "    #         X_test = [imageMatrix_iso0.png, imageMatrix_iso0.png, ........., imageMatrix_iso18402.png  ]\n",
    "    #         X_val = [imageMatrix_iso0.png, imageMatrix_iso0.png, ........., imageMatrix_iso18402.png ]\n",
    "    #--------------------------------------------------------------------------------------------------------#\n",
    "    def get_all_images_Matrix(self):\n",
    "        directories = [trainDatasImmagesDir, testDatasImmagesDir, valDatasImmagesDir]\n",
    "        if not self.gt:\n",
    "            print(\"Error no ground truth to prevent to load junk image\")\n",
    "            return\n",
    "        for index, data_dir_abs_path in enumerate(directories):\n",
    "            print(\"loding image ..............\\n\")\n",
    "            if os.path.isdir(data_dir_abs_path):\n",
    "                if index == 0:\n",
    "                    slice_start, slice_stop = 3, -4\n",
    "                    ground_truth = self.gt['train_gt']\n",
    "                if index == 1:\n",
    "                    slice_start, slice_stop = 3, -4\n",
    "                    ground_truth = self.gt['test_gt']\n",
    "                if index == 2:\n",
    "                    slice_start, slice_stop = 4, -4\n",
    "                    ground_truth = self.gt['val_gt']\n",
    "\n",
    "                print(\".... start loading  in folder: {}\".format(data_dir_abs_path))\n",
    "                #get array of files\n",
    "                images_files_array = sorted(glob.glob(data_dir_abs_path + os.sep + \"*.png\"))\n",
    "               \n",
    "                #sort files array\n",
    "                images_files_array = sorted(images_files_array, key=lambda name: int(\n",
    "                (os.path.basename(name))[slice_start: slice_stop]))\n",
    "                \n",
    "                #inkml_files = inkml_files.sort(key=self.sortGlobFilesArray)\n",
    "                for j, image_file_abs_path in enumerate(images_files_array):\n",
    "                    \n",
    "                    file_name_index = image_file_abs_path.split('/')[-1]\n",
    "                    file_name_index = file_name_index[int(slice_start): int(slice_stop)]\n",
    "                    #print(\".............fle check: {} .............\".format(file_name_index))\n",
    "                    file_is_junk = True\n",
    "                    for key in ground_truth:\n",
    "                        if file_name_index in ground_truth[key]:\n",
    "                            file_is_junk = False\n",
    "                    if  False == file_is_junk:\n",
    "                        imageMatrix = self.get_matrix_from_image(image_file_abs_path).reshape(28, 28, 1)\n",
    "               \n",
    "                        if index == 0:\n",
    "                            self.X_train.append(imageMatrix)\n",
    "                        if index == 1:\n",
    "                            self.X_test.append(imageMatrix)\n",
    "                        if index == 2:\n",
    "                            self.X_val.append(imageMatrix)\n",
    "                    #else: \n",
    "                        #print('...............Found JUNK FILE .....................\\n')\n",
    "                if index == 0:\n",
    "                    print(\"------- LEN IMAGES FILES: \", len(self.X_train))\n",
    "                if index == 1:\n",
    "                    print(\"------- LEN IMAGES FILES: \", len(self.X_test))\n",
    "                if index == 2:\n",
    "                    print(\"------- LEN IMAGES FILES: \", len(self.X_val))\n",
    "\n",
    "            else:\n",
    "                print(\"Error: External Devices not found\\n\")\n",
    "        \n",
    "        \n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    #  Result: y_train = Matrix 101 x 101, y_test = Matrix 101 x 101, y_val = Matrix 101 x 101\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    def get_all_images_categorical(self):\n",
    "        gt_directories_files = [trainDir_ground_truth, testDir_ground_truth, valDir_ground_truth]\n",
    "        imagesDir = [trainDatasImmagesDir, testDatasImmagesDir, valDatasImmagesDir]\n",
    "        \n",
    "        for index, abs_gt_file_path in enumerate(gt_directories_files):\n",
    "            #print(\"Start reading grount truth ..... {}\" .format(os.path.basename(abs_gt_file_path)))\n",
    "            sorted_gt, total_character, y = self.read_gt(abs_gt_file_path, imagesDir[index])\n",
    "            \n",
    "            length = len(sorted_gt.keys())\n",
    "            \n",
    "            for i, el in enumerate(y):\n",
    "                x = np.zeros(length, dtype=np.float32)\n",
    "                x[el] = 1.0\n",
    "                        \n",
    "                if index == 0:\n",
    "                    self.y_train.append(x)\n",
    "                    self.gt['train_gt'] = sorted_gt\n",
    "                    self.y_train_ordered_labels = sorted_gt.keys()\n",
    "                if index == 1:\n",
    "                    self.y_test.append(x)\n",
    "                    self.gt['test_gt'] = sorted_gt\n",
    "                    self.y_test_ordered_labels = sorted_gt.keys()\n",
    "                if index == 2:\n",
    "                    self.y_val.append(x)\n",
    "                    self.gt['val_gt'] = sorted_gt\n",
    "                    self.y_val_ordered_labels = sorted_gt.keys()\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------------#\n",
    "    # return a Dictionary\n",
    "    # {\n",
    "    # ')' : [abs_path_image1, abs_path_imag2, ...]\n",
    "    #    .....................................\n",
    "    #   '!'  : [abs_path_image14, abs_path_imag267, ...]\n",
    "    # }\n",
    "    #------------------------------------------------------------------------------#\n",
    "    def read_gt(self, abs_gt_file_path, imagesDir):\n",
    "        '''Loads the routes of the png files'''\n",
    "        try:\n",
    "            fileName = os.path.basename(abs_gt_file_path)\n",
    "            print(\"reading ground truth {} ..................\".format(fileName))\n",
    "            with open(abs_gt_file_path) as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "                aDict = {}\n",
    "                aLabelsSet = []\n",
    "                total_character = 0\n",
    "                all_labels = []\n",
    "                \n",
    "                for index, line in enumerate(lines):\n",
    "                    line = line.strip()\n",
    "\n",
    "                    parts = line.split(',')\n",
    "                    label = parts[1].strip()\n",
    "\n",
    "                    data = parts[0].split('_')\n",
    "                    index = data[-1].strip()\n",
    "                    if(label != 'junk'):\n",
    "                        all_labels.append(label)\n",
    "                    if label not in aLabelsSet and label != 'junk':\n",
    "                        aLabelsSet.append(label)\n",
    "\n",
    "                    if label in aDict and label != 'junk':\n",
    "                        #aDict[label].append(imagesDir + '/' + 'iso' + index + '.png')\n",
    "                        aDict[label].append(index)\n",
    "                    else:\n",
    "                        #aDict[label] = [imagesDir + '/' + 'iso' + index + '.png']\n",
    "                        aDict[label] = [index]\n",
    "                    total_character = total_character + 1\n",
    "\n",
    "            #sort dictionary but others method doesn't work\n",
    "            bDict = {}\n",
    "            y = []\n",
    "            for label in aLabelsSet:\n",
    "                bDict[label] = aDict[label]\n",
    "            for label in all_labels:\n",
    "                for i, el in enumerate(aLabelsSet):\n",
    "                    if(el == label):\n",
    "                        y.append(i)\n",
    "\n",
    "            #aDict = OrderedDict(sorted(aDict.items(), key=lambda t: t[0]))\n",
    "            # print(\"dictionnary sorted keys : \", bDict.keys())\n",
    "            print(\"Finish readin ground {} truth: {}\".format(fileName, len(bDict)))\n",
    "            #print(\"bDict length:........\", len(bDict))\n",
    "            return bDict, total_character,y\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(InputLayer(input_shape = (28, 28, 1)  ))\n",
    "        #adding layers\n",
    "        model.add(Conv2D(64, (3,3), padding='same', activation='relu', strides=2, kernel_initializer='truncated_normal'))\n",
    "        model.add(MaxPooling2D( (3, 3), strides = 1, padding='same' ) )\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=1,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(MaxPooling2D( (3, 3), strides = 1, padding='same' ) )\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=1,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=1,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=1,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=1,\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(UpSampling2D((2, 2)))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(UpSampling2D((2, 2)))\n",
    "        model.add(Conv2D(32, (3, 3), activation=None, padding='same',\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(Conv2D(2, (3, 3), activation=None, padding='same',\n",
    "                 kernel_initializer='truncated_normal'))\n",
    "        model.add(UpSampling2D((2, 2)))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        #compile model using accuracy to measure model performance\n",
    "        # Compile the model\n",
    "        #model.compile(\n",
    "            #loss=keras.losses.mean_squared_error, optimizer='sgd')\n",
    "        optimizer = tf.keras.optimizers.RMSprop( lr = 0.0005, decay = 1e-5 )\n",
    "        model.compile( optimizer=optimizer, loss='mse', metrics=['acc'] )\n",
    "        #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model):\n",
    "        #fit the model\n",
    "        #model.fit(self.X_train, self.y_train, batch_size=500, epochs=5)\n",
    "        \n",
    "        #model.fit(self.X_train, self.y_train, validation_data=(self.X_test, self.y_test), epochs=5,batch_size=500)\n",
    "        # Evaluate the model on test set\n",
    "        \n",
    "        #score=model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        \n",
    "        # Print test accuracy\n",
    "        #print('\\nTest accuracy: {}'.format(score[1]*100))\n",
    "        model.fit( x=self.X_train, y=self.y_train, validation_data=(self.X_test, self.y_test), batch_size=1, epochs=1800)\n",
    "        return model\n",
    "    \n",
    "    def save_model(self, model):\n",
    "        # save model and architecture to single file\n",
    "        model.save(models_save + \"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        return model\n",
    "    \n",
    "    def load_saved_model(self, model):\n",
    "        model = load_model(models_save + \"model.h5\")\n",
    "        # summarize model.\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        score = model.evaluate(self.X_val, self.y_val, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    def model_predict(self, model, start):\n",
    "        #predict first 4 images in the test set\n",
    "        return model.predict(self.X_val[start : start + 1])\n",
    "        \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ground truth iso_GT.txt ..................\n",
      "Finish readin ground iso_GT.txt truth: 101\n",
      "reading ground truth testSymbols_2016_iso_GT.txt ..................\n",
      "Finish readin ground testSymbols_2016_iso_GT.txt truth: 101\n",
      "reading ground truth iso_GT.txt ..................\n",
      "Finish readin ground iso_GT.txt truth: 101\n",
      "loding image ..............\n",
      "\n",
      ".... start loading  in folder: /Volumes/Disk E/crohme2016_inkml_images/train_images\n",
      "------- LEN IMAGES FILES:  85802\n",
      "loding image ..............\n",
      "\n",
      ".... start loading  in folder: /Volumes/Disk E/crohme2016_inkml_images/test_images\n",
      "------- LEN IMAGES FILES:  10019\n",
      "loding image ..............\n",
      "\n",
      ".... start loading  in folder: /Volumes/Disk E/crohme2016_inkml_images/val_images\n",
      "------- LEN IMAGES FILES:  6083\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['(', '-', ')', 'n', '=', 'i', '\\\\sum', 'S', '1', 'r', '\\\\theta', '2', '\\\\pi', '8', 'd', 'a', '4', 'c', 'b', '+', '3', '7', 'y', '0', 'x', '\\\\sqrt', 'k', '\\\\sin', '\\\\cos', '\\\\pm', 'p', 'u', 'q', 'P', 'j', '\\\\leq', '|', '\\\\lt', '!', '\\\\ldots', 'f', 'C', 'B', 'A', 'R', 'm', 'COMMA', 't', '\\\\prime', 'w', 's', '\\\\int', '\\\\alpha', 'H', '\\\\Delta', 'I', '\\\\in', 'M', 'X', '\\\\sigma', 'T', 'E', 'L', '\\\\forall', 'V', '\\\\mu', '\\\\{', '\\\\}', 'N', '\\\\neq', 'G', 'e', 'F', 'g', '\\\\phi', '\\\\exists', '\\\\gt', 'z', '\\\\log', '9', '\\\\div', '5', '\\\\times', 'o', 'v', '6', '/', '\\\\gamma', '\\\\beta', 'h', '\\\\geq', 'Y', '\\\\infty', 'l', '.', '\\\\rightarrow', '\\\\lim', '[', ']', '\\\\tan', '\\\\lambda'])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.y_train_ordered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'k', '+', 'y', '4', '8', '\\\\sqrt', '2', '6', 'q', 't', '=', '-', 'p', 'e', '1', ')', '(', 'd', '0', 'm', '\\\\geq', 'u', '\\\\theta', '3', '\\\\neq', 'n', 'C', '\\\\ldots', '\\\\gamma', '\\\\div', '5', '7', '9', 'COMMA', '\\\\int', 'g', '\\\\rightarrow', '\\\\infty', '\\\\lim', 'a', 'b', '\\\\sin', 'L', '\\\\leq', 'c', '\\\\cos', 'o', 'R', 'r', '\\\\beta', '.', 'S', 'I', 'X', '\\\\pi', '\\\\mu', 'v', '\\\\times', 'Y', '/', 'f', 'z', '\\\\sum', '\\\\alpha', 'F', '\\\\log', '\\\\prime', 'T', 'i', 'B', 'l', ']', '[', '\\\\exists', 'w', 'H', 'A', 'M', '\\\\pm', 'N', 'V', 'P', 'j', 'E', '|', '\\\\lt', '\\\\tan', '\\\\gt', '\\\\phi', 's', '\\\\Delta', '!', '\\\\{', '\\\\}', '\\\\sigma', 'G', '\\\\forall', '\\\\in', '\\\\lambda', 'h'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.y_test_ordered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'n', '1', '+', '-', 'X', '\\\\ldots', '=', 'P', '0', 'c', 'v', '(', ')', 'e', 'w', 'k', 'COMMA', 'x', 'd', 'p', 'T', '\\\\Delta', 'G', 'z', 'R', 'g', 'A', '.', 'm', 'q', 'u', 's', '\\\\pi', '3', '6', '2', 'N', 'i', 'E', 'B', 'S', '\\\\times', 'F', 'f', 'r', 'M', 't', '\\\\prime', 'y', '/', 'b', '\\\\gt', 'l', '\\\\mu', 'L', 'j', 'H', '\\\\infty', '\\\\int', '\\\\sqrt', '|', 'C', '\\\\lt', '4', '5', '\\\\leq', 'I', '\\\\alpha', '\\\\gamma', '\\\\sigma', '8', '\\\\in', 'o', '\\\\{', '\\\\}', ']', '[', '\\\\beta', 'V', '7', '\\\\theta', '\\\\log', '\\\\geq', 'h', '\\\\cos', '\\\\neq', '\\\\sin', '\\\\sum', '\\\\tan', '\\\\phi', '\\\\rightarrow', '\\\\lim', 'Y', '9', '\\\\pm', '!', '\\\\exists', '\\\\forall', '\\\\lambda', '\\\\div'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.y_val_ordered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-09843a51f513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-59c9acf255b7>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;31m#adding layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'truncated_normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "model = cnn.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn.compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85802 samples, validate on 10019 samples\n",
      "Epoch 1/1800\n",
      " 1601/85802 [..............................] - ETA: 9:24 - loss: 1.2907 - accuracy: 0.6615"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-7012e274979b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-47c56ce8cee9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Print test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m#print('\\nTest accuracy: {}'.format(score[1]*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m       args, kwargs = self._function_spec.canonicalize_function_inputs(\n\u001b[0;32m-> 2661\u001b[0;31m           *args, **kwargs)\n\u001b[0m\u001b[1;32m   2662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/machine_learning_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2178\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2179\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = cnn.train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model_predict() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-873a6e0d68de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: model_predict() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "cnn.model_predict(model ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.66862635e-10, 9.95224703e-09, 1.38151590e-09, 1.50686334e-04,\n",
       "        3.41216388e-09, 1.10437199e-07, 1.39720173e-08, 1.18959204e-10,\n",
       "        7.91695318e-04, 5.77257606e-06, 1.33835567e-06, 5.60225279e-04,\n",
       "        4.31640814e-07, 1.05810315e-07, 2.04762764e-05, 9.84200776e-01,\n",
       "        1.29356061e-03, 1.24378012e-05, 5.51517099e-09, 1.60859895e-06,\n",
       "        4.62428972e-07, 1.08008644e-05, 3.21167227e-06, 2.53103872e-06,\n",
       "        5.15664369e-03, 1.46104240e-10, 8.30289082e-06, 1.16428043e-06,\n",
       "        6.07252014e-06, 1.10353762e-08, 6.74697409e-09, 4.50776797e-03,\n",
       "        7.88829402e-06, 9.07459707e-09, 4.66488936e-09, 6.58876047e-08,\n",
       "        1.94051353e-09, 2.55968331e-07, 1.70574932e-09, 1.52992081e-11,\n",
       "        9.52555812e-10, 1.86429361e-07, 1.39081786e-08, 5.09674874e-06,\n",
       "        5.57849307e-06, 2.13248882e-06, 4.70390980e-08, 4.27921691e-07,\n",
       "        3.07570467e-08, 5.52212036e-08, 1.25047023e-07, 8.00979616e-10,\n",
       "        2.94296653e-03, 7.83644438e-08, 6.18273987e-07, 6.49563381e-09,\n",
       "        6.62601423e-08, 4.86292720e-07, 1.11716297e-07, 2.79565370e-07,\n",
       "        1.74643563e-10, 7.58537053e-11, 1.01053681e-08, 1.59806890e-09,\n",
       "        7.99556261e-08, 9.16450244e-06, 1.11812437e-09, 2.83898416e-08,\n",
       "        1.40219409e-08, 2.08189199e-08, 1.58259936e-05, 1.24917060e-04,\n",
       "        1.41532938e-10, 2.15415824e-07, 3.52150391e-06, 2.14350804e-09,\n",
       "        8.80896134e-10, 9.73397437e-06, 4.60250476e-06, 7.96455060e-06,\n",
       "        4.65235794e-10, 1.97691574e-09, 1.93223883e-07, 1.09981420e-06,\n",
       "        3.10296900e-06, 6.20225693e-09, 2.85393348e-10, 2.39098359e-08,\n",
       "        9.07577302e-09, 1.26083978e-06, 1.35232492e-08, 2.40708919e-09,\n",
       "        6.29139720e-07, 2.59996523e-05, 8.64719914e-05, 1.13728811e-10,\n",
       "        1.97141631e-07, 2.23898222e-12, 1.18744197e-08, 4.83873457e-08,\n",
       "        2.02558022e-06]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(cnn.X_val[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
